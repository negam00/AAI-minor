{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, GRU\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a corpus of 323669963 characters\n"
     ]
    }
   ],
   "source": [
    "with open(\"lyrics.csv\") as corpus_file:\n",
    "    corpus = corpus_file.read()\n",
    "    corpus = corpus.lower()\n",
    "print(\"Loaded a corpus of {0} characters\".format(len(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\x01', '\\x02', '\\x03', '\\x04', '\\x05', '\\x06', '\\x07', '\\x08', '\\t', '\\n', '\\x0b', '\\x0c', '\\x0e', '\\x0f', '\\x10', '\\x11', '\\x12', '\\x13', '\\x14', '\\x15', '\\x16', '\\x17', '\\x18', '\\x19', '\\x1a', '\\x1b', '\\x1c', '\\x1d', '\\x1e', '\\x1f', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x7f', '\\x80', '\\x81', '\\x82', '\\x83', '\\x84', '\\x85', '\\x86', '\\x87', '\\x88', '\\x89', '\\x8a', '\\x8b', '\\x8c', '\\x8d', '\\x8e', '\\x8f', '\\x90', '\\x91', '\\x92', '\\x93', '\\x94', '\\x95', '\\x96', '\\x97', '\\x98', '\\x99', '\\x9a', '\\x9b', '\\x9c', '\\x9d', '\\x9e', '\\x9f', '\\xa0', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '\\xad', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', '×', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ó', 'ô', 'õ', 'ö', 'ø', 'ù', 'ú', 'û', 'ü', 'þ', 'ă', 'ą', 'ć', 'č', 'ę', 'ł', 'ń', 'œ', 'ś', 'š', 'ź', 'ż', 'ž', '’', '∧', '≠', '\\u3000', 'あ', 'い', 'う', 'か', 'が', 'き', 'く', 'け', 'げ', 'こ', 'ご', 'さ', 'ざ', 'し', 'じ', 'す', 'ず', 'せ', 'そ', 'た', 'だ', 'っ', 'つ', 'づ', 'て', 'で', 'と', 'ど', 'な', 'に', 'ぬ', 'の', 'は', 'び', 'ま', 'み', 'も', 'ゃ', 'や', 'よ', 'ら', 'り', 'る', 'れ', 'ろ', 'わ', 'を', 'ん', 'ウ', 'オ', 'ジ', 'テ', 'ビ', 'ラ', 'レ', 'ー', '上', '下', '乗', '乱', '事', '人', '代', '会', '位', '何', '僕', '八', '共', '凪', '出', '分', '切', '別', '前', '千', '友', '名', '呼', '命', '咲', '唄', '塗', '変', '夕', '大', '嬉', '宇', '宙', '宝', '少', '届', '島', '嵐', '幸', '度', '悲', '愛', '揺', '教', '散', '星', '映', '時', '書', '来', '森', '歌', '永', '汚', '波', '流', '海', '涙', '減', '渡', '物', '珊', '瑚', '生', '知', '砂', '神', '私', '科', '空', '繰', '聞', '花', '行', '見', '誰', '輝', '返', '送', '遠', '雲', '風', '魚', '鳥', '�']\n",
      "Our corpus contains 352 unique characters.\n",
      "{'\\x01': 0, '\\x02': 1, '\\x03': 2, '\\x04': 3, '\\x05': 4, '\\x06': 5, '\\x07': 6, '\\x08': 7, '\\t': 8, '\\n': 9, '\\x0b': 10, '\\x0c': 11, '\\x0e': 12, '\\x0f': 13, '\\x10': 14, '\\x11': 15, '\\x12': 16, '\\x13': 17, '\\x14': 18, '\\x15': 19, '\\x16': 20, '\\x17': 21, '\\x18': 22, '\\x19': 23, '\\x1a': 24, '\\x1b': 25, '\\x1c': 26, '\\x1d': 27, '\\x1e': 28, '\\x1f': 29, ' ': 30, '!': 31, '\"': 32, '#': 33, '$': 34, '%': 35, '&': 36, \"'\": 37, '(': 38, ')': 39, '*': 40, '+': 41, ',': 42, '-': 43, '.': 44, '/': 45, '0': 46, '1': 47, '2': 48, '3': 49, '4': 50, '5': 51, '6': 52, '7': 53, '8': 54, '9': 55, ':': 56, ';': 57, '<': 58, '=': 59, '>': 60, '?': 61, '@': 62, '[': 63, '\\\\': 64, ']': 65, '^': 66, '_': 67, '`': 68, 'a': 69, 'b': 70, 'c': 71, 'd': 72, 'e': 73, 'f': 74, 'g': 75, 'h': 76, 'i': 77, 'j': 78, 'k': 79, 'l': 80, 'm': 81, 'n': 82, 'o': 83, 'p': 84, 'q': 85, 'r': 86, 's': 87, 't': 88, 'u': 89, 'v': 90, 'w': 91, 'x': 92, 'y': 93, 'z': 94, '{': 95, '|': 96, '}': 97, '~': 98, '\\x7f': 99, '\\x80': 100, '\\x81': 101, '\\x82': 102, '\\x83': 103, '\\x84': 104, '\\x85': 105, '\\x86': 106, '\\x87': 107, '\\x88': 108, '\\x89': 109, '\\x8a': 110, '\\x8b': 111, '\\x8c': 112, '\\x8d': 113, '\\x8e': 114, '\\x8f': 115, '\\x90': 116, '\\x91': 117, '\\x92': 118, '\\x93': 119, '\\x94': 120, '\\x95': 121, '\\x96': 122, '\\x97': 123, '\\x98': 124, '\\x99': 125, '\\x9a': 126, '\\x9b': 127, '\\x9c': 128, '\\x9d': 129, '\\x9e': 130, '\\x9f': 131, '\\xa0': 132, '¡': 133, '¢': 134, '£': 135, '¤': 136, '¥': 137, '¦': 138, '§': 139, '¨': 140, '©': 141, 'ª': 142, '«': 143, '¬': 144, '\\xad': 145, '®': 146, '¯': 147, '°': 148, '±': 149, '²': 150, '³': 151, '´': 152, 'µ': 153, '¶': 154, '·': 155, '¸': 156, '¹': 157, 'º': 158, '»': 159, '¼': 160, '½': 161, '¾': 162, '¿': 163, '×': 164, 'ß': 165, 'à': 166, 'á': 167, 'â': 168, 'ã': 169, 'ä': 170, 'å': 171, 'æ': 172, 'ç': 173, 'è': 174, 'é': 175, 'ê': 176, 'ë': 177, 'ì': 178, 'í': 179, 'î': 180, 'ï': 181, 'ð': 182, 'ñ': 183, 'ó': 184, 'ô': 185, 'õ': 186, 'ö': 187, 'ø': 188, 'ù': 189, 'ú': 190, 'û': 191, 'ü': 192, 'þ': 193, 'ă': 194, 'ą': 195, 'ć': 196, 'č': 197, 'ę': 198, 'ł': 199, 'ń': 200, 'œ': 201, 'ś': 202, 'š': 203, 'ź': 204, 'ż': 205, 'ž': 206, '’': 207, '∧': 208, '≠': 209, '\\u3000': 210, 'あ': 211, 'い': 212, 'う': 213, 'か': 214, 'が': 215, 'き': 216, 'く': 217, 'け': 218, 'げ': 219, 'こ': 220, 'ご': 221, 'さ': 222, 'ざ': 223, 'し': 224, 'じ': 225, 'す': 226, 'ず': 227, 'せ': 228, 'そ': 229, 'た': 230, 'だ': 231, 'っ': 232, 'つ': 233, 'づ': 234, 'て': 235, 'で': 236, 'と': 237, 'ど': 238, 'な': 239, 'に': 240, 'ぬ': 241, 'の': 242, 'は': 243, 'び': 244, 'ま': 245, 'み': 246, 'も': 247, 'ゃ': 248, 'や': 249, 'よ': 250, 'ら': 251, 'り': 252, 'る': 253, 'れ': 254, 'ろ': 255, 'わ': 256, 'を': 257, 'ん': 258, 'ウ': 259, 'オ': 260, 'ジ': 261, 'テ': 262, 'ビ': 263, 'ラ': 264, 'レ': 265, 'ー': 266, '上': 267, '下': 268, '乗': 269, '乱': 270, '事': 271, '人': 272, '代': 273, '会': 274, '位': 275, '何': 276, '僕': 277, '八': 278, '共': 279, '凪': 280, '出': 281, '分': 282, '切': 283, '別': 284, '前': 285, '千': 286, '友': 287, '名': 288, '呼': 289, '命': 290, '咲': 291, '唄': 292, '塗': 293, '変': 294, '夕': 295, '大': 296, '嬉': 297, '宇': 298, '宙': 299, '宝': 300, '少': 301, '届': 302, '島': 303, '嵐': 304, '幸': 305, '度': 306, '悲': 307, '愛': 308, '揺': 309, '教': 310, '散': 311, '星': 312, '映': 313, '時': 314, '書': 315, '来': 316, '森': 317, '歌': 318, '永': 319, '汚': 320, '波': 321, '流': 322, '海': 323, '涙': 324, '減': 325, '渡': 326, '物': 327, '珊': 328, '瑚': 329, '生': 330, '知': 331, '砂': 332, '神': 333, '私': 334, '科': 335, '空': 336, '繰': 337, '聞': 338, '花': 339, '行': 340, '見': 341, '誰': 342, '輝': 343, '返': 344, '送': 345, '遠': 346, '雲': 347, '風': 348, '魚': 349, '鳥': 350, '�': 351}\n"
     ]
    }
   ],
   "source": [
    "# Get a unique identifier for each char in the corpus, then make some dicts to ease encoding and decoding\n",
    "chars = sorted(list(set(corpus)))\n",
    "num_chars = len(chars)\n",
    "encoding = {c: i for i, c in enumerate(chars)}\n",
    "decoding = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "print(chars)\n",
    "print(\"Our corpus contains {0} unique characters.\".format(num_chars))\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-41f730e1f6f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msentence_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msentence_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mX_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0my_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_char\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-41f730e1f6f8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msentence_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msentence_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mX_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0my_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_char\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# it slices, it dices, it makes julienned datasets!\n",
    "# chop up our data into X and y, slice into roughly (num_chars / skip) overlapping 'sentences'\n",
    "# of length sentence_length, and encode the chars\n",
    "sentence_length = 20\n",
    "skip = 1\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range (0, len(corpus) - sentence_length, skip):\n",
    "    sentence = corpus[i:i + sentence_length]\n",
    "    next_char = corpus[i + sentence_length]\n",
    "    X_data.append([encoding[char] for char in sentence])\n",
    "    y_data.append(encoding[next_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(len(X_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X_data[1], y_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X_data[2], y_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "num_sentences = len(X_data)\n",
    "print(\"Sliced our corpus into {0} sentences of length {1}\".format(num_sentences, sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Vectorizing X and y...\")\n",
    "X = np.zeros((num_sentences, sentence_length, num_chars), dtype=np.bool)\n",
    "y = np.zeros((num_sentences, num_chars), dtype=np.bool)\n",
    "for i, sentence in enumerate(X_data):\n",
    "    for t, encoded_char in enumerate(sentence):\n",
    "        X[i, t, encoded_char] = 1\n",
    "    y[i, y_data[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Double check our vectorized data before we sink hours into fitting a model\n",
    "print(\"Sanity check y. Dimension: {0} # Sentences: {1} Characters in corpus: {2}\".format(y.shape, num_sentences, len(chars)))\n",
    "print(\"Sanity check X. Dimension: {0} Sentence length: {1}\".format(X.shape, sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Define our model\n",
    "print(\"Let's build model 1\")\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(sentence_length, num_chars)))\n",
    "model.add(Dense(num_chars))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Dump our model architecture to a file so we can load it elsewhere\n",
    "# Find out how to load a model? ,\n",
    "# return_sequences=True\n",
    "architecture = model.to_yaml()\n",
    "with open('model.yaml', 'a') as model_file:\n",
    "    model_file.write(architecture)\n",
    "\n",
    "# Set up checkpoints, and save trained model\n",
    "file_path=\"weights-{epoch:02d}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor=\"loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "# Find out how to load the trained checkpoint?\n",
    "# Lets go, action time!\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#model.add(GRU(256),return_sequences=True)\n",
    "#model.add(GRU(256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
